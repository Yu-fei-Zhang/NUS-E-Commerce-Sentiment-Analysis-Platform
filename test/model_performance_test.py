"""
Model Performance Test Tool
"""

import ast
import os
import time
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from TorchCRF import CRF
from transformers import BertTokenizerFast, BertModel
from seqeval.metrics import classification_report, f1_score, precision_score, recall_score
import pandas as pd
import numpy as np
from typing import List, Dict, Tuple
import json
import tracemalloc
import psutil


import numpy as np


# ==================== è¾…åŠ©å‡½æ•° ====================
def convert_to_serializable(obj):
    """
    é€’å½’è½¬æ¢å¯¹è±¡ä¸ºJSONå¯åºåˆ—åŒ–çš„ç±»å‹
    å¤„ç† numpy ç±»å‹å’Œå…¶ä»–ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
    """
    if isinstance(obj, dict):
        return {key: convert_to_serializable(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_to_serializable(item) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(convert_to_serializable(item) for item in obj)
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, (np.bool_, bool)):
        return bool(obj)
    else:
        return obj


# ==================== æ¨¡å‹å®šä¹‰ ====================
class Config:
    def __init__(self):
        self.model_name = 'bert-base-chinese'
        self.max_len = 128
        self.batch_size = 32
        self.dropout = 0.1
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.labels = [
            'O',
            'B-ASP', 'I-ASP',
            'B-SENT_POS', 'I-SENT_POS',
            'B-SENT_NEG', 'I-SENT_NEG',
            'B-SENT_NEU', 'I-SENT_NEU'
        ]
        self.label2id = {label: i for i, label in enumerate(self.labels)}
        self.id2label = {i: label for i, label in enumerate(self.labels)}
        self.num_labels = len(self.labels)
        self.output_dir = '../saved_model/best_model'


class BertCRFModel(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.bert = BertModel.from_pretrained(config.model_name)
        self.dropout = nn.Dropout(config.dropout)
        # ä½¿ç”¨ hidden2tag ä»¥åŒ¹é…è®­ç»ƒæ—¶çš„æ¨¡å‹ç»“æ„
        self.hidden2tag = nn.Linear(self.bert.config.hidden_size, config.num_labels)
        self.crf = CRF(num_tags=config.num_labels, batch_first=True)

    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        last_hidden_state = outputs.last_hidden_state
        logits = self.hidden2tag(self.dropout(last_hidden_state))

        # Create CRF mask based on attention_mask
        crf_mask = attention_mask.bool()

        if labels is not None:
            # Create a version of labels where -100 is replaced with a valid tag index
            valid_labels = labels.clone()
            valid_labels[valid_labels == -100] = 0  # Replace -100 with 'O' tag index

            # Calculate CRF loss
            loss = -self.crf(logits, valid_labels, mask=crf_mask)
            predictions = self.crf.decode(logits, mask=crf_mask)
            return loss.mean(), predictions
        else:
            # For decoding
            pred_tags = self.crf.decode(logits, mask=crf_mask)
            return pred_tags


class AspectSentimentDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len, label2id):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.label2id = label2id

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        char_labels = self.labels[idx]

        if not isinstance(text, str) or not text.strip():
            text = "å ä½ç¬¦"
            char_labels = ['O']

        if not isinstance(char_labels, list):
            char_labels = ['O'] * len(text)

        if len(char_labels) != len(text):
            if len(char_labels) < len(text):
                char_labels += ['O'] * (len(text) - len(char_labels))
            else:
                char_labels = char_labels[:len(text)]

        encoding = self.tokenizer(
            text,
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_offsets_mapping=True,
            return_tensors='pt'
        )

        input_ids = encoding['input_ids'].flatten()
        attention_mask = encoding['attention_mask'].flatten()
        offset_mapping = encoding['offset_mapping'].squeeze(0).numpy()

        token_labels = [-100] * len(input_ids)
        for token_idx, (start, end) in enumerate(offset_mapping):
            if start == 0 and end == 0:
                token_labels[token_idx] = -100
            else:
                if start < len(char_labels):
                    label = char_labels[start]
                else:
                    label = 'O'
                token_label = self.label2id.get(label, self.label2id['O'])
                token_labels[token_idx] = token_label

        labels_tensor = torch.tensor(token_labels, dtype=torch.long)

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels_tensor,
            'text': text
        }


# ==================== æ€§èƒ½æµ‹è¯•ç±» ====================
class ModelPerformanceTester:
    def __init__(self, model_path: str, config: Config):
        self.config = config
        self.device = config.device
        self.tokenizer = BertTokenizerFast.from_pretrained(config.model_name)

        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

        self.model = BertCRFModel(config).to(self.device)
        if os.path.exists(model_path):
            self.model.load_state_dict(torch.load(model_path, map_location=self.device))
            print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
        else:
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")

        self.model.eval()

    def test_inference_speed(self, test_texts: List[str], num_runs: int = 100) -> Dict:
        """æµ‹è¯•æ¨ç†é€Ÿåº¦"""
        print("\n" + "="*60)
        print("1. æ¨ç†é€Ÿåº¦æµ‹è¯•")
        print("="*60)

        results = {
            'single_inference': [],
            'batch_inference': []
        }

        # å•æ ·æœ¬æ¨ç†é€Ÿåº¦æµ‹è¯•
        print("\n[1.1] å•æ ·æœ¬æ¨ç†é€Ÿåº¦æµ‹è¯•")
        test_text = test_texts[0] if test_texts else "è¿™å®¶åº—çš„è¡£æœé¢æ–™å·®ï¼Œä½†ç‰ˆå‹å¾ˆå¥½"

        # é¢„çƒ­
        for _ in range(10):
            self._infer_single(test_text)

        # æ­£å¼æµ‹è¯•
        times = []
        for i in range(num_runs):
            start = time.time()
            self._infer_single(test_text)
            end = time.time()
            times.append((end - start) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’

        results['single_inference'] = {
            'mean_ms': np.mean(times),
            'median_ms': np.median(times),
            'std_ms': np.std(times),
            'min_ms': np.min(times),
            'max_ms': np.max(times),
            'p95_ms': np.percentile(times, 95),
            'p99_ms': np.percentile(times, 99)
        }

        print(f"  å¹³å‡æ¨ç†æ—¶é—´: {results['single_inference']['mean_ms']:.2f} ms")
        print(f"  ä¸­ä½æ•°æ—¶é—´: {results['single_inference']['median_ms']:.2f} ms")
        print(f"  95åˆ†ä½æ•°: {results['single_inference']['p95_ms']:.2f} ms")
        print(f"  99åˆ†ä½æ•°: {results['single_inference']['p99_ms']:.2f} ms")

        # æ‰¹é‡æ¨ç†é€Ÿåº¦æµ‹è¯•
        if len(test_texts) >= 32:
            print("\n[1.2] æ‰¹é‡æ¨ç†é€Ÿåº¦æµ‹è¯• (batch_size=32)")
            batch_texts = test_texts[:32]

            # é¢„çƒ­
            for _ in range(5):
                self._infer_batch(batch_texts)

            # æ­£å¼æµ‹è¯•
            batch_times = []
            for i in range(20):
                start = time.time()
                self._infer_batch(batch_texts)
                end = time.time()
                batch_times.append((end - start) * 1000)

            results['batch_inference'] = {
                'mean_ms': np.mean(batch_times),
                'median_ms': np.median(batch_times),
                'throughput': 32 / (np.mean(batch_times) / 1000)  # æ ·æœ¬/ç§’
            }

            print(f"  æ‰¹é‡å¹³å‡æ—¶é—´: {results['batch_inference']['mean_ms']:.2f} ms")
            print(f"  ååé‡: {results['batch_inference']['throughput']:.2f} æ ·æœ¬/ç§’")

        return results

    def test_accuracy(self, test_data_path: str = None,
                     test_texts: List[str] = None,
                     test_labels: List[List[str]] = None) -> Dict:
        """æµ‹è¯•æ¨¡å‹å‡†ç¡®ç‡"""
        print("\n" + "="*60)
        print("2. å‡†ç¡®ç‡æµ‹è¯•")
        print("="*60)

        if test_data_path:
            print(f"\nä»æ–‡ä»¶åŠ è½½æµ‹è¯•æ•°æ®: {test_data_path}")
            test_texts, test_labels = self._load_test_data(test_data_path)
        elif test_texts and test_labels:
            print(f"\nä½¿ç”¨æä¾›çš„æµ‹è¯•æ•°æ®: {len(test_texts)} æ¡")
        else:
            print("\næœªæä¾›æµ‹è¯•æ•°æ®ï¼Œè·³è¿‡å‡†ç¡®ç‡æµ‹è¯•")
            return {}

        # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
        test_dataset = AspectSentimentDataset(
            test_texts, test_labels, self.tokenizer,
            self.config.max_len, self.config.label2id
        )
        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

        # è¯„ä¼°
        all_true_tags = []
        all_pred_tags = []

        with torch.no_grad():
            for batch in test_loader:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels']

                predictions = self.model(input_ids, attention_mask)

                for j in range(len(labels)):
                    true_seq_ids = labels[j].cpu().numpy()
                    pred_seq_ids = predictions[j]

                    true_seq = []
                    pred_seq = []

                    effective_length = min(len(true_seq_ids), len(pred_seq_ids))
                    for k in range(effective_length):
                        true_id = true_seq_ids[k]
                        pred_id = pred_seq_ids[k]

                        if true_id != -100:
                            if 0 <= true_id < len(self.config.id2label):
                                true_tag = self.config.id2label[true_id]
                            else:
                                true_tag = 'O'
                            true_seq.append(true_tag)

                            if 0 <= pred_id < len(self.config.id2label):
                                pred_tag = self.config.id2label[pred_id]
                            else:
                                pred_tag = 'O'
                            pred_seq.append(pred_tag)

                    if true_seq and pred_seq:
                        all_true_tags.append(true_seq)
                        all_pred_tags.append(pred_seq)

        if not all_true_tags:
            print("è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•æ•°æ®")
            return {}

        # è®¡ç®—æŒ‡æ ‡
        print("\n[2.1] æ•´ä½“æ€§èƒ½æŒ‡æ ‡:")
        report = classification_report(all_true_tags, all_pred_tags, output_dict=True)

        metrics = {
            'micro_f1': f1_score(all_true_tags, all_pred_tags),
            'precision': precision_score(all_true_tags, all_pred_tags),
            'recall': recall_score(all_true_tags, all_pred_tags)
        }

        print(f"  Micro F1: {metrics['micro_f1']:.4f}")
        print(f"  Precision: {metrics['precision']:.4f}")
        print(f"  Recall: {metrics['recall']:.4f}")

        # å„ç±»åˆ«æ€§èƒ½
        print("\n[2.2] å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡:")
        print(classification_report(all_true_tags, all_pred_tags))

        return {
            'metrics': metrics,
            'detailed_report': report
        }

    def test_memory_usage(self, test_texts: List[str]) -> Dict:
        """æµ‹è¯•å†…å­˜ä½¿ç”¨"""
        print("\n" + "="*60)
        print("3. å†…å­˜ä½¿ç”¨æµ‹è¯•")
        print("="*60)

        process = psutil.Process()

        # è·å–æ¨¡å‹å‚æ•°é‡
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        print(f"\n[3.1] æ¨¡å‹å‚æ•°é‡:")
        print(f"  æ€»å‚æ•°é‡: {total_params:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"  æ¨¡å‹å¤§å°: {total_params * 4 / 1024 / 1024:.2f} MB (FP32)")

        # æµ‹è¯•æ¨ç†å†…å­˜
        print(f"\n[3.2] æ¨ç†å†…å­˜å ç”¨:")
        tracemalloc.start()
        mem_before = process.memory_info().rss / 1024 / 1024

        # æ‰§è¡Œæ¨ç†
        test_text = test_texts[0] if test_texts else "è¿™å®¶åº—çš„è¡£æœé¢æ–™å·®ï¼Œä½†ç‰ˆå‹å¾ˆå¥½"
        for _ in range(10):
            self._infer_single(test_text)

        mem_after = process.memory_info().rss / 1024 / 1024
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()

        print(f"  æ¨ç†å‰å†…å­˜: {mem_before:.2f} MB")
        print(f"  æ¨ç†åå†…å­˜: {mem_after:.2f} MB")
        print(f"  å†…å­˜å¢é‡: {mem_after - mem_before:.2f} MB")
        print(f"  å³°å€¼å†…å­˜: {peak / 1024 / 1024:.2f} MB")

        return {
            'total_params': total_params,
            'trainable_params': trainable_params,
            'model_size_mb': total_params * 4 / 1024 / 1024,
            'inference_memory_mb': mem_after - mem_before,
            'peak_memory_mb': peak / 1024 / 1024
        }

    def test_robustness(self, base_text: str = "è¿™å®¶åº—çš„è¡£æœé¢æ–™å·®ï¼Œä½†ç‰ˆå‹å¾ˆå¥½") -> Dict:
        """æµ‹è¯•æ¨¡å‹é²æ£’æ€§"""
        print("\n" + "="*60)
        print("4. é²æ£’æ€§æµ‹è¯•")
        print("="*60)

        test_cases = {
            'åŸå§‹æ–‡æœ¬': base_text,
            'çŸ­æ–‡æœ¬': "å¾ˆå¥½",
            'é•¿æ–‡æœ¬': base_text * 5,
            'ç‰¹æ®Šå­—ç¬¦': base_text + "!@#$%^&*()",
            'æ•°å­—æ··åˆ': base_text + "123456",
            'è‹±æ–‡æ··åˆ': base_text + "very good",
            'ç©ºæ ¼æ–‡æœ¬': "  " + base_text + "  ",
            'é‡å¤æ–‡æœ¬': "å•†å“è´¨é‡å¥½å¥½å¥½å¥½å¥½éå¸¸å¥½"

        }

        results = {}
        print("\n[4.1] ä¸åŒè¾“å…¥æµ‹è¯•:")

        for name, text in test_cases.items():
            try:
                start = time.time()
                result = self._infer_single(text)
                elapsed = (time.time() - start) * 1000

                # æå–æƒ…æ„Ÿç»“æœ
                aspects = [r for r in result if r['type'] == 'aspect']
                sentiments = [r for r in result if r['type'] == 'sentiment']

                results[name] = {
                    'success': True,
                    'time_ms': elapsed,
                    'aspects_found': len(aspects),
                    'sentiments_found': len(sentiments),
                    'text_length': len(text)
                }
                print(f"  âœ“ {name}: {elapsed:.2f}ms | æ–¹é¢:{len(aspects)} æƒ…æ„Ÿ:{len(sentiments)}")
            except Exception as e:
                results[name] = {
                    'success': False,
                    'error': str(e)
                }
                print(f"  âœ— {name}: å¤±è´¥ - {str(e)}")

        return results

    def test_edge_cases(self) -> Dict:
        """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""
        print("\n" + "="*60)
        print("5. è¾¹ç•Œæƒ…å†µæµ‹è¯•")
        print("="*60)

        edge_cases = {
            'ç©ºæ–‡æœ¬': "",
            'å•å­—': "å¥½",
            'çº¯æ ‡ç‚¹': "ã€‚ã€‚ã€‚ï¼ï¼ï¼",
            'è¶…é•¿æ–‡æœ¬': "å•†å“è´¨é‡" + "éå¸¸" * 100 + "å¥½",
            'çº¯æ•°å­—': "12345678",
            'çº¯è‹±æ–‡': "very good quality",
            'ç‰¹æ®ŠUnicode': "ğŸ˜Šå•†å“è´¨é‡ğŸ‘ğŸ‰",
            'æ¢è¡Œæ–‡æœ¬': "ç¬¬ä¸€è¡Œ\nç¬¬äºŒè¡Œå•†å“è´¨é‡å¾ˆå¥½\nç¬¬ä¸‰è¡Œ"
        }

        results = {}
        print("\n[5.1] è¾¹ç•Œæƒ…å†µæµ‹è¯•:")

        for name, text in edge_cases.items():
            try:
                result = self._infer_single(text)
                results[name] = {
                    'success': True,
                    'result_count': len(result)
                }
                print(f"  âœ“ {name}: æˆåŠŸ (ç»“æœæ•°: {len(result)})")
            except Exception as e:
                results[name] = {
                    'success': False,
                    'error': str(e)
                }
                print(f"  âœ— {name}: {str(e)[:50]}")

        return results

    def _infer_single(self, text: str) -> List[Dict]:
        """å•æ ·æœ¬æ¨ç†"""
        encoding = self.tokenizer(
            text,
            max_length=self.config.max_len,
            padding='max_length',
            truncation=True,
            return_offsets_mapping=True,
            return_tensors='pt'
        )

        input_ids = encoding['input_ids'].to(self.device)
        attention_mask = encoding['attention_mask'].to(self.device)
        offset_mapping = encoding['offset_mapping'].squeeze(0).cpu().numpy()

        with torch.no_grad():
            pred_tags = self.model(input_ids, attention_mask)[0]

        # æå–ç»“æœ
        results = []
        current_entity = None

        for idx, (start, end) in enumerate(offset_mapping):
            if start == end:
                continue

            if idx < len(pred_tags):
                tag_id = pred_tags[idx]
                if 0 <= tag_id < len(self.config.id2label):
                    tag = self.config.id2label[tag_id]

                    if tag.startswith('B-'):
                        if current_entity:
                            results.append(current_entity)

                        entity_type = 'aspect' if 'ASP' in tag else 'sentiment'
                        sentiment = None
                        if 'POS' in tag:
                            sentiment = 'positive'
                        elif 'NEG' in tag:
                            sentiment = 'negative'
                        elif 'NEU' in tag:
                            sentiment = 'neutral'

                        current_entity = {
                            'text': text[start:end],
                            'start': int(start),
                            'end': int(end),
                            'type': entity_type,
                            'sentiment': sentiment
                        }
                    elif tag.startswith('I-') and current_entity:
                        current_entity['text'] += text[start:end]
                        current_entity['end'] = int(end)
                    elif tag == 'O' and current_entity:
                        results.append(current_entity)
                        current_entity = None

        if current_entity:
            results.append(current_entity)

        return results

    def _infer_batch(self, texts: List[str]):
        """æ‰¹é‡æ¨ç†"""
        encodings = self.tokenizer(
            texts,
            max_length=self.config.max_len,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        input_ids = encodings['input_ids'].to(self.device)
        attention_mask = encodings['attention_mask'].to(self.device)

        with torch.no_grad():
            self.model(input_ids, attention_mask)

    def _load_test_data(self, file_path: str) -> Tuple[List[str], List[List[str]]]:
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        df = pd.read_csv(file_path)
        df = df.dropna(subset=['char_labels', 'content'])
        df = df[df['content'].str.strip().astype(bool)]

        def safe_parse(x):
            try:
                return ast.literal_eval(x) if isinstance(x, str) else x
            except:
                return None

        df['char_labels'] = df['char_labels'].apply(safe_parse)
        df = df[df['char_labels'].apply(lambda x: isinstance(x, list))]
        df['content'] = df['content'].astype(str)

        texts = df['content'].tolist()
        labels = df['char_labels'].tolist()

        return texts, labels

    def run_all_tests(self, test_data_path: str = None,
                      test_texts: List[str] = None,
                      test_labels: List[List[str]] = None,
                      save_report: bool = True) -> Dict:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("\n" + "="*80)
        print(" "*20 + "æ¨¡å‹æ€§èƒ½ç»¼åˆæµ‹è¯•æŠ¥å‘Š")
        print("="*80)

        all_results = {}

        # å‡†å¤‡æµ‹è¯•æ–‡æœ¬
        if not test_texts:
            test_texts = [
                "è¿™å®¶åº—çš„è¡£æœé¢æ–™å·®ï¼Œä½†ç‰ˆå‹å¾ˆå¥½",
                "æœåŠ¡æ€åº¦å¾ˆå¥½ï¼Œç¯å¢ƒä¹Ÿä¸é”™",
                "ä»·æ ¼å¤ªè´µäº†ï¼Œæ€§ä»·æ¯”ä¸é«˜",
                "èœå“å‘³é“ä¸€èˆ¬ï¼Œä½†æ˜¯åˆ†é‡å¾ˆè¶³"
            ] * 10  # åˆ›å»ºè¶³å¤Ÿçš„æµ‹è¯•æ•°æ®

        # 1. æ¨ç†é€Ÿåº¦æµ‹è¯•
        all_results['inference_speed'] = self.test_inference_speed(test_texts)

        # 2. å‡†ç¡®ç‡æµ‹è¯•
        if test_data_path or (test_texts and test_labels):
            all_results['accuracy'] = self.test_accuracy(test_data_path, test_texts, test_labels)

        # 3. å†…å­˜ä½¿ç”¨æµ‹è¯•
        all_results['memory'] = self.test_memory_usage(test_texts)

        # 4. é²æ£’æ€§æµ‹è¯•
        all_results['robustness'] = self.test_robustness()

        # 5. è¾¹ç•Œæƒ…å†µæµ‹è¯•
        all_results['edge_cases'] = self.test_edge_cases()

        # ç”Ÿæˆæ€»ç»“
        print("\n" + "="*80)
        print(" "*30 + "æµ‹è¯•æ€»ç»“")
        print("="*80)

        if 'inference_speed' in all_results and 'single_inference' in all_results['inference_speed']:
            speed = all_results['inference_speed']['single_inference']
            print(f"\nâœ“ æ¨ç†é€Ÿåº¦: å¹³å‡ {speed['mean_ms']:.2f} ms/æ ·æœ¬")

        if 'accuracy' in all_results and 'metrics' in all_results['accuracy']:
            acc = all_results['accuracy']['metrics']
            print(f"âœ“ æ¨¡å‹å‡†ç¡®ç‡: F1={acc['micro_f1']:.4f}, P={acc['precision']:.4f}, R={acc['recall']:.4f}")

        if 'memory' in all_results:
            mem = all_results['memory']
            print(f"âœ“ å†…å­˜ä½¿ç”¨: æ¨¡å‹{mem['model_size_mb']:.2f}MB, æ¨ç†å³°å€¼{mem['peak_memory_mb']:.2f}MB")

        if 'robustness' in all_results:
            robust = all_results['robustness']
            success_count = sum(1 for r in robust.values() if r.get('success', False))
            print(f"âœ“ é²æ£’æ€§: {success_count}/{len(robust)} æµ‹è¯•é€šè¿‡")

        if 'edge_cases' in all_results:
            edge = all_results['edge_cases']
            success_count = sum(1 for r in edge.values() if r.get('success', False))
            print(f"âœ“ è¾¹ç•Œæµ‹è¯•: {success_count}/{len(edge)} æµ‹è¯•é€šè¿‡")

        print("\n" + "="*80)

        # ä¿å­˜æŠ¥å‘Š
        if save_report:
            report_path = 'performance_test_report.json'  # ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œä¿å­˜åœ¨å½“å‰ç›®å½•
            # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
            serializable_results = convert_to_serializable(all_results)
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(serializable_results, f, indent=2, ensure_ascii=False)
            print(f"\nè¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")

        return all_results


# ==================== ä¸»å‡½æ•° ====================
def main():
    # é…ç½®
    config = Config()
    model_path = os.path.join(config.output_dir, 'previous_model.pt')

    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    if not os.path.exists(model_path):
        print(f"é”™è¯¯: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        print("è¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹è·¯å¾„")
        return

    # åˆ›å»ºæµ‹è¯•å™¨
    tester = ModelPerformanceTester(model_path, config)

    # è¿è¡Œæµ‹è¯• (å¯ä»¥æä¾›æµ‹è¯•æ•°æ®è·¯å¾„)
    test_data_path = '../data/test_dataset.csv'  # å¦‚æœ‰æµ‹è¯•æ•°æ®
    results = tester.run_all_tests(
        test_data_path=test_data_path,  # å–æ¶ˆæ³¨é‡Šä»¥ä½¿ç”¨æµ‹è¯•æ•°æ®
        save_report=True
    )

    return results


if __name__ == "__main__":
    main()