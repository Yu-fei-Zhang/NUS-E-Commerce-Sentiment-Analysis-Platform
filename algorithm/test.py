"""
æµ‹è¯•è„šæœ¬
ç”¨äºæµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬é²æ£’æ€§æµ‹è¯•å’Œè¾¹ç•Œæµ‹è¯•
"""
import torch
from transformers import BertTokenizerFast
from model import BertCRFModel
from config import Config
from inference import predict, format_results


def test_positive_samples():
    """æµ‹è¯•æ­£å‘è¯„è®ºè¯†åˆ«"""
    print("\n" + "=" * 80)
    print("æ­£å‘è¯„è®ºè¯†åˆ«æµ‹è¯•")
    print("=" * 80)

    positive_samples = [
        "è¿™æœ¬ä¹¦å†…å®¹éå¸¸å¥½ï¼Œå¾ˆæœ‰æ·±åº¦",
        "æ‰‹æœºå±å¹•æ¸…æ™°ï¼Œæ€§èƒ½å¼ºå¤§",
        "æœåŠ¡æ€åº¦å¾ˆå¥½ï¼Œç¯å¢ƒèˆ’é€‚",
        "è´¨é‡ä¸Šä¹˜ï¼Œä»·æ ¼åˆç†",
        "ç‰©æµé€Ÿåº¦å¿«ï¼ŒåŒ…è£…ç²¾ç¾",
        "å‘³é“é²œç¾ï¼Œåˆ†é‡åè¶³",
        "æ“ä½œç®€å•ï¼ŒåŠŸèƒ½é½å…¨",
        "è®¾è®¡ç²¾ç¾ï¼Œåšå·¥è€ƒç©¶"
    ]

    return positive_samples


def test_negative_samples():
    """æµ‹è¯•è´Ÿå‘è¯„è®ºè¯†åˆ«"""
    print("\n" + "=" * 80)
    print("è´Ÿå‘è¯„è®ºè¯†åˆ«æµ‹è¯•")
    print("=" * 80)

    negative_samples = [
        "è¿™æœ¬ä¹¦å†…å®¹å¾ˆå·®ï¼Œæ²¡æœ‰ä»·å€¼",
        "æ‰‹æœºå±å¹•æ¨¡ç³Šï¼Œæ€§èƒ½å¾ˆçƒ‚",
        "æœåŠ¡æ€åº¦æ¶åŠ£ï¼Œç¯å¢ƒè„ä¹±",
        "è´¨é‡ä½åŠ£ï¼Œä»·æ ¼è™šé«˜",
        "ç‰©æµé€Ÿåº¦æ…¢ï¼ŒåŒ…è£…ç ´æŸ",
        "å‘³é“éš¾åƒï¼Œåˆ†é‡å¾ˆå°‘",
        "æ“ä½œå¤æ‚ï¼ŒåŠŸèƒ½ç¼ºå¤±",
        "è®¾è®¡ä¸‘é™‹ï¼Œåšå·¥ç²—ç³™"
    ]

    return negative_samples


def test_mixed_samples():
    """æµ‹è¯•æ··åˆæƒ…æ„Ÿè¯†åˆ«"""
    print("\n" + "=" * 80)
    print("æ··åˆæƒ…æ„Ÿè¯†åˆ«æµ‹è¯•")
    print("=" * 80)

    mixed_samples = [
        "è¿™å®¶åº—çš„è¡£æœé¢æ–™å·®ï¼Œä½†ç‰ˆå‹å¾ˆå¥½",
        "æ‰‹æœºå±å¹•å¾ˆæ¸…æ™°ï¼Œä½†ç”µæ± ç»­èˆªä¸è¡Œ",
        "é…’åº—æœåŠ¡æ€åº¦å¾ˆå¥½ï¼Œä½†ç¯å¢ƒä¸€èˆ¬",
        "è¿™æœ¬ä¹¦å†…å®¹å¾ˆå……å®ï¼Œä½†æ˜¯ä»·æ ¼æœ‰ç‚¹è´µ",
        "é¤å…çš„èœå“å‘³é“ä¸€èˆ¬ï¼Œä½†æ˜¯ç¯å¢ƒä¸é”™",
        "è´¨é‡è¿˜è¡Œï¼Œä½†ä»·æ ¼å¤ªè´µäº†",
        "å¤–è§‚è®¾è®¡æ¼‚äº®ï¼Œä½†åŠŸèƒ½å¾ˆå°‘",
        "ç‰©æµå¾ˆå¿«ï¼Œä½†åŒ…è£…ç®€é™‹"
    ]

    return mixed_samples


def test_robustness():
    """é²æ£’æ€§æµ‹è¯•ï¼šåŒ…å«å™ªå£°ã€é”™åˆ«å­—ç­‰"""
    print("\n" + "=" * 80)
    print("é²æ£’æ€§æµ‹è¯•")
    print("=" * 80)

    robustness_samples = [
        "è¿™ä¸ªæ‰‹æœºçœŸçš„å¤ªæ£’äº†ï¼ï¼ï¼",  # é‡å¤æ ‡ç‚¹
        "æœåŠ¡æ€åº¦éå¸¸éå¸¸éå¸¸å¥½",  # é‡å¤è¯è¯­
        "å‘³é“çœŸæ˜¯å¥½æäº†~~~",  # ç‰¹æ®Šç¬¦å·
        "è³ªé‡å¾ˆå¥½ï¼Œåƒ¹æ ¼åˆç†",  # ç¹ä½“å­—
        "å±å¹•æ¸…æ™°åº¦çœŸçš„æ²¡è¯è¯´",  # å£è¯­åŒ–è¡¨è¾¾
        "ä¹°çš„å¾ˆå€¼å¾—ï¼Œå¼ºçƒˆæ¨èğŸ‘",  # è¡¨æƒ…ç¬¦å·
        "æ€§èƒ½æ æ çš„ï¼Œå®Œå…¨æ»¡è¶³éœ€æ±‚",  # ç½‘ç»œç”¨è¯­
        "ä¸œè¥¿æ”¶åˆ°äº†ï¼Œè¿˜ä¸é”™å“¦~"  # å£è¯­åŒ–
    ]

    return robustness_samples


def test_boundary_cases():
    """è¾¹ç•Œæƒ…å†µæµ‹è¯•"""
    print("\n" + "=" * 80)
    print("è¾¹ç•Œæƒ…å†µæµ‹è¯•")
    print("=" * 80)

    boundary_samples = [
        "è¿˜è¡Œ",  # æçŸ­æ–‡æœ¬
        "ä¸é”™",  # æçŸ­æ–‡æœ¬
        "ä¸€èˆ¬èˆ¬",  # æ¨¡ç³Šè¯„ä»·
        "é©¬é©¬è™è™",  # æ¨¡ç³Šè¯„ä»·
        "æ— åŠŸæ— è¿‡",  # ä¸­æ€§è¯„ä»·
        "è´¨é‡",  # å•ä¸ªå±æ€§è¯
        "å¥½",  # å•ä¸ªæƒ…æ„Ÿè¯
        "è¿™ä¸ªè¿™ä¸ªè¿™ä¸ªçœŸçš„å¾ˆå¥½",  # é‡å¤è¯
        "æˆ‘è§‰å¾—å¯èƒ½å¤§æ¦‚ä¹Ÿè®¸åº”è¯¥è¿˜ä¸é”™å§",  # ä¸ç¡®å®šè¡¨è¾¾
    ]

    return boundary_samples


def test_long_text():
    """æµ‹è¯•é•¿æ–‡æœ¬"""
    print("\n" + "=" * 80)
    print("é•¿æ–‡æœ¬æµ‹è¯•")
    print("=" * 80)

    long_samples = [
        "è¿™æœ¬ä¹¦çš„å†…å®¹éå¸¸ä¸°å¯Œï¼Œä½œè€…çš„å†™ä½œé£æ ¼å¾ˆç‹¬ç‰¹ï¼Œä½†æ˜¯ä»·æ ¼ç¡®å®æœ‰ç‚¹è´µã€‚ä¸è¿‡è€ƒè™‘åˆ°ä¹¦ç±çš„è´¨é‡å’Œè£…å¸§ï¼Œè¿˜æ˜¯ç‰©æœ‰æ‰€å€¼çš„ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘è§‰å¾—è¿™æ˜¯ä¸€æœ¬å€¼å¾—æ¨èçš„å¥½ä¹¦ï¼Œå°½ç®¡æœ‰äº›ç« èŠ‚ç•¥æ˜¾å†—é•¿ã€‚",
        "æ‰‹æœºçš„å±å¹•æ˜¾ç¤ºæ•ˆæœéå¸¸æ¸…æ™°ï¼Œè‰²å½©è¿˜åŸåº¦å¾ˆé«˜ï¼Œçœ‹è§†é¢‘ä½“éªŒå¾ˆå¥½ã€‚ç”µæ± ç»­èˆªèƒ½åŠ›ä¹Ÿä¸é”™ï¼Œæ­£å¸¸ä½¿ç”¨ä¸€å¤©å®Œå…¨æ²¡é—®é¢˜ã€‚ä½†æ˜¯æ‘„åƒå¤´çš„å¤œæ‹æ•ˆæœä¸€èˆ¬ï¼Œåœ¨å…‰çº¿ä¸å¥½çš„æƒ…å†µä¸‹å™ªç‚¹æ¯”è¾ƒæ˜æ˜¾ã€‚æ•´ä½“æ¥è¯´æ€§ä»·æ¯”è¿˜æ˜¯æŒºé«˜çš„ã€‚"
    ]

    return long_samples


def run_all_tests(model, tokenizer, config):
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""

    test_cases = {
        "æ­£å‘è¯„è®º": test_positive_samples(),
        "è´Ÿå‘è¯„è®º": test_negative_samples(),
        "æ··åˆæƒ…æ„Ÿ": test_mixed_samples(),
        "é²æ£’æ€§": test_robustness(),
        "è¾¹ç•Œæƒ…å†µ": test_boundary_cases(),
        "é•¿æ–‡æœ¬": test_long_text()
    }

    for test_name, samples in test_cases.items():
        print("\n" + "=" * 80)
        print(f"{test_name}æµ‹è¯•")
        print("=" * 80)

        for i, text in enumerate(samples, 1):
            print(f"\næµ‹è¯•æ ·ä¾‹ {i}:")
            results = predict(text, model, tokenizer, config)
            print(format_results(text, results))

            # åˆ†æç»“æœ
            analyze_prediction(text, results, test_name)


def analyze_prediction(text: str, results: list, test_type: str):
    """åˆ†æé¢„æµ‹ç»“æœ"""
    if not results:
        print("âš ï¸  è­¦å‘Š: æœªè¯†åˆ«åˆ°ä»»ä½•ç»“æœ")
        return

    has_aspect = any(r['aspect_phrase'] for r in results)
    has_sentiment = any(r['sentiment_phrase'] for r in results)
    has_positive = any(r['sentiment'] == 'æ­£å‘' for r in results)
    has_negative = any(r['sentiment'] == 'è´Ÿå‘' for r in results)

    print("\nåˆ†æ:")
    print(f"  - è¯†åˆ«åˆ°å±æ€§: {'âœ“' if has_aspect else 'âœ—'}")
    print(f"  - è¯†åˆ«åˆ°æƒ…æ„Ÿ: {'âœ“' if has_sentiment else 'âœ—'}")

    if has_sentiment:
        if has_positive:
            print(f"  - æ­£å‘æƒ…æ„Ÿ: âœ“")
        if has_negative:
            print(f"  - è´Ÿå‘æƒ…æ„Ÿ: âœ“")

    # æ ¹æ®æµ‹è¯•ç±»å‹è¿›è¡Œç‰¹å®šåˆ†æ
    if test_type == "æ­£å‘è¯„è®º" and not has_positive:
        print("  âš ï¸  è­¦å‘Š: æ­£å‘è¯„è®ºæœªè¯†åˆ«åˆ°æ­£å‘æƒ…æ„Ÿ!")
    elif test_type == "è´Ÿå‘è¯„è®º" and not has_negative:
        print("  âš ï¸  è­¦å‘Š: è´Ÿå‘è¯„è®ºæœªè¯†åˆ«åˆ°è´Ÿå‘æƒ…æ„Ÿ!")
    elif test_type == "æ··åˆæƒ…æ„Ÿ" and not (has_positive and has_negative):
        print("  âš ï¸  æç¤º: æ··åˆæƒ…æ„Ÿå¯èƒ½æœªå®Œå…¨è¯†åˆ«")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    # åŠ è½½é…ç½®
    config = Config()
    print(config)

    # åŠ è½½tokenizer
    print("\nåŠ è½½Tokenizer...")
    tokenizer = BertTokenizerFast.from_pretrained(config.model_name)

    # åŠ è½½æ¨¡å‹
    print("\nåŠ è½½æ¨¡å‹...")
    model_path = f"{config.output_dir}/best_model"

    try:
        model = BertCRFModel.load_pretrained(model_path, device=config.device)
        print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ!")
    except Exception as e:
        print(f"âœ— æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("è¯·å…ˆè¿è¡Œ train.py è®­ç»ƒæ¨¡å‹")
        return

    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    run_all_tests(model, tokenizer, config)

    print("\n" + "=" * 80)
    print("æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
    print("=" * 80)


if __name__ == "__main__":
    main()